{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From: [OpenNMT/OpenNMT-py: Open Source Neural Machine Translation in PyTorch](https://github.com/OpenNMT/OpenNMT-py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_relative_positions_matrix(length, max_relative_positions,\n",
    "                                       cache=False):\n",
    "    \"\"\"Generate the clipped relative positions matrix\n",
    "       for a given length and maximum relative positions\"\"\"\n",
    "    if cache:\n",
    "        distance_mat = torch.arange(-length+1, 1, 1).unsqueeze(0)\n",
    "    else:\n",
    "        range_vec = torch.arange(length)\n",
    "        range_mat = range_vec.unsqueeze(-1).expand(-1, length).transpose(0, 1)\n",
    "        distance_mat = range_mat - range_mat.transpose(0, 1)\n",
    "    distance_mat_clipped = torch.clamp(distance_mat,\n",
    "                                       min=-max_relative_positions,\n",
    "                                       max=max_relative_positions)\n",
    "    # Shift values to be >= 0\n",
    "    final_mat = distance_mat_clipped + max_relative_positions\n",
    "    return final_mat\n",
    "\n",
    "\n",
    "def relative_matmul(x, z, transpose):\n",
    "    \"\"\"Helper function for relative positions attention.\"\"\"\n",
    "    batch_size = x.shape[0]\n",
    "    heads = x.shape[1]\n",
    "    length = x.shape[2]\n",
    "    x_t = x.permute(2, 0, 1, 3)\n",
    "    x_t_r = x_t.reshape(length, heads * batch_size, -1)\n",
    "    if transpose:\n",
    "        z_t = z.transpose(1, 2)\n",
    "        x_tz_matmul = torch.matmul(x_t_r, z_t)\n",
    "    else:\n",
    "        x_tz_matmul = torch.matmul(x_t_r, z)\n",
    "    x_tz_matmul_r = x_tz_matmul.reshape(length, batch_size, heads, -1)\n",
    "    x_tz_matmul_r_t = x_tz_matmul_r.permute(1, 2, 0, 3)\n",
    "    return x_tz_matmul_r_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultiHead Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, head_count, model_dim, dropout=0.1,\n",
    "                 max_relative_positions=0):\n",
    "        assert model_dim % head_count == 0\n",
    "        self.dim_per_head = model_dim // head_count\n",
    "        self.model_dim = model_dim\n",
    "\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        self.head_count = head_count\n",
    "\n",
    "        self.linear_keys = nn.Linear(model_dim,\n",
    "                                     head_count * self.dim_per_head)\n",
    "        self.linear_values = nn.Linear(model_dim,\n",
    "                                       head_count * self.dim_per_head)\n",
    "        self.linear_query = nn.Linear(model_dim,\n",
    "                                      head_count * self.dim_per_head)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.final_linear = nn.Linear(model_dim, model_dim)\n",
    "\n",
    "        self.max_relative_positions = max_relative_positions\n",
    "\n",
    "        if max_relative_positions > 0:\n",
    "            vocab_size = max_relative_positions * 2 + 1\n",
    "            self.relative_positions_embeddings = nn.Embedding(\n",
    "                vocab_size, self.dim_per_head)\n",
    "\n",
    "    def forward(self, key, value, query, mask=None, layer_cache=None, attn_type=None):\n",
    "        batch_size = key.size(0)\n",
    "        dim_per_head = self.dim_per_head\n",
    "        head_count = self.head_count\n",
    "        key_len = key.size(1)\n",
    "        query_len = query.size(1)\n",
    "\n",
    "        def shape(x):\n",
    "            \"\"\"Projection.\"\"\"\n",
    "            return x.view(batch_size, -1, head_count, dim_per_head) \\\n",
    "                .transpose(1, 2)\n",
    "\n",
    "        def unshape(x):\n",
    "            \"\"\"Compute context.\"\"\"\n",
    "            return x.transpose(1, 2).contiguous() \\\n",
    "                    .view(batch_size, -1, head_count * dim_per_head)\n",
    "\n",
    "        # 1) Project key, value, and query.\n",
    "        if layer_cache is not None:\n",
    "            if attn_type == \"self\":\n",
    "                query, key, value = self.linear_query(query),\\\n",
    "                                    self.linear_keys(query),\\\n",
    "                                    self.linear_values(query)\n",
    "                key = shape(key)\n",
    "                value = shape(value)\n",
    "                if layer_cache[\"self_keys\"] is not None:\n",
    "                    key = torch.cat(\n",
    "                        (layer_cache[\"self_keys\"], key),\n",
    "                        dim=2)\n",
    "                if layer_cache[\"self_values\"] is not None:\n",
    "                    value = torch.cat(\n",
    "                        (layer_cache[\"self_values\"], value),\n",
    "                        dim=2)\n",
    "                layer_cache[\"self_keys\"] = key\n",
    "                layer_cache[\"self_values\"] = value\n",
    "            elif attn_type == \"context\":\n",
    "                query = self.linear_query(query)\n",
    "                if layer_cache[\"memory_keys\"] is None:\n",
    "                    key, value = self.linear_keys(key),\\\n",
    "                                 self.linear_values(value)\n",
    "                    key = shape(key)\n",
    "                    value = shape(value)\n",
    "                else:\n",
    "                    key, value = layer_cache[\"memory_keys\"],\\\n",
    "                               layer_cache[\"memory_values\"]\n",
    "                layer_cache[\"memory_keys\"] = key\n",
    "                layer_cache[\"memory_values\"] = value\n",
    "        else:\n",
    "            key = self.linear_keys(key)\n",
    "            value = self.linear_values(value)\n",
    "            query = self.linear_query(query)\n",
    "            key = shape(key)\n",
    "            value = shape(value)\n",
    "\n",
    "        if self.max_relative_positions > 0 and attn_type == \"self\":\n",
    "            key_len = key.size(2)\n",
    "            # 1 or key_len x key_len\n",
    "            relative_positions_matrix = generate_relative_positions_matrix(\n",
    "                key_len, self.max_relative_positions,\n",
    "                cache=True if layer_cache is not None else False)\n",
    "            #  1 or key_len x key_len x dim_per_head\n",
    "            relations_keys = self.relative_positions_embeddings(\n",
    "                relative_positions_matrix.to(key.device))\n",
    "            #  1 or key_len x key_len x dim_per_head\n",
    "            relations_values = self.relative_positions_embeddings(\n",
    "                relative_positions_matrix.to(key.device))\n",
    "\n",
    "        query = shape(query)\n",
    "\n",
    "        key_len = key.size(2)\n",
    "        query_len = query.size(2)\n",
    "\n",
    "        # 2) Calculate and scale scores.\n",
    "        query = query / math.sqrt(dim_per_head)\n",
    "        # batch x num_heads x query_len x key_len\n",
    "        query_key = torch.matmul(query, key.transpose(2, 3))\n",
    "\n",
    "        if self.max_relative_positions > 0 and attn_type == \"self\":\n",
    "            scores = query_key + relative_matmul(query, relations_keys, True)\n",
    "        else:\n",
    "            scores = query_key\n",
    "        scores = scores.float()\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)  # [B, 1, 1, T_values]\n",
    "            scores = scores.masked_fill(mask, -1e18)\n",
    "\n",
    "        # 3) Apply attention dropout and compute context vectors.\n",
    "        attn = self.softmax(scores).to(query.dtype)\n",
    "        drop_attn = self.dropout(attn)\n",
    "\n",
    "        context_original = torch.matmul(drop_attn, value)\n",
    "\n",
    "        if self.max_relative_positions > 0 and attn_type == \"self\":\n",
    "            context = unshape(context_original\n",
    "                              + relative_matmul(drop_attn,\n",
    "                                                relations_values,\n",
    "                                                False))\n",
    "        else:\n",
    "            context = unshape(context_original)\n",
    "\n",
    "        output = self.final_linear(context)\n",
    "\n",
    "        # Return multi-head attn\n",
    "        attns = attn.view(batch_size, head_count, query_len, key_len)\n",
    "        return output, attns\n",
    "\n",
    "    def update_dropout(self, dropout):\n",
    "        self.dropout.p = dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention2(nn.Module):\n",
    "    def __init__(self, head_count, model_dim, dropout=0.1):\n",
    "        assert model_dim % head_count == 0\n",
    "        self.dim_per_head = model_dim // head_count\n",
    "        self.model_dim = model_dim\n",
    "\n",
    "        super(MultiHeadedAttention2, self).__init__()\n",
    "        self.head_count = head_count\n",
    "\n",
    "        self.linear_keys = nn.Linear(model_dim, head_count * self.dim_per_head)\n",
    "        self.linear_values = nn.Linear(model_dim, head_count * self.dim_per_head)\n",
    "        self.linear_query = nn.Linear(model_dim, head_count * self.dim_per_head)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.final_linear = nn.Linear(model_dim, model_dim)\n",
    "    def forward(self, key, value, query, mask=None):\n",
    "        batch_size = key.size(0)\n",
    "        dim_per_head = self.dim_per_head\n",
    "        head_count = self.head_count\n",
    "        key_len = key.size(1)\n",
    "        query_len = query.size(1)\n",
    "\n",
    "        def shape(x):\n",
    "            \"\"\"Projection.\"\"\"\n",
    "            return x.view(batch_size, -1, head_count, dim_per_head).transpose(1, 2)\n",
    "\n",
    "        def unshape(x):\n",
    "            \"\"\"Compute context.\"\"\"\n",
    "            return x.transpose(1, 2).contiguous().view(\n",
    "                batch_size, -1, head_count * dim_per_head)\n",
    "\n",
    "        # 1) Project key, value, and query.\n",
    "        key = self.linear_keys(key)\n",
    "        value = self.linear_values(value)\n",
    "        query = self.linear_query(query)\n",
    "        key = shape(key)\n",
    "        value = shape(value)\n",
    "        query = shape(query)\n",
    "\n",
    "        key_len = key.size(2)\n",
    "        query_len = query.size(2)\n",
    "\n",
    "        # 2) Calculate and scale scores.\n",
    "        query = query / math.sqrt(dim_per_head)\n",
    "        # batch x num_heads x query_len x key_len\n",
    "        query_key = torch.matmul(query, key.transpose(2, 3))\n",
    "        scores = query_key\n",
    "        scores = scores.float()\n",
    "\n",
    "        # 3) Apply attention dropout and compute context vectors.\n",
    "        attn = self.softmax(scores).to(query.dtype)\n",
    "        drop_attn = self.dropout(attn)\n",
    "        print(drop_attn.shape, value.shape)\n",
    "        context_original = torch.matmul(drop_attn, value)\n",
    "        print(context_original.shape)\n",
    "        context = unshape(context_original)\n",
    "        output = self.final_linear(context)\n",
    "        print(output.shape)\n",
    "        # Return multi-head attn\n",
    "        print(attn.shape)\n",
    "        attns = attn.view(batch_size, head_count, query_len, key_len)\n",
    "        print(attns.shape)\n",
    "        return output, attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "mh2 = MultiHeadedAttention2(1, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 10, 10]) torch.Size([1, 1, 10, 128])\n",
      "torch.Size([1, 1, 10, 128])\n",
      "torch.Size([1, 10, 128])\n",
      "torch.Size([1, 1, 10, 10])\n",
      "torch.Size([1, 1, 10, 10])\n"
     ]
    }
   ],
   "source": [
    "output2, att2 = mh2(input, input, input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 128])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = nn.Linear(128, 128)(input)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 10, 8])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.view(1, -1, 16, 8).transpose(1, 2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 128])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_.transpose(1, 2).contiguous().view(1, -1, 16 * 8).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Embedding(1000, 64)(input).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.LongTensor([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (batch, key_len, dim)\n",
    "input = torch.randn(1, 10, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "mh = MultiHeadedAttention(16, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "output, attn = mh(input, input, input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 128])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 10, 10])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Position Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"Implement the PE function.\"\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + Variable(self.pe[:, :x.size(1)], \n",
    "                         requires_grad=False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2,  4,  6,  8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36,\n",
       "        38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70, 72,\n",
       "        74, 76, 78, 80, 82, 84, 86, 88, 90, 92, 94, 96, 98])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(2, 100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 10\n",
    "dim = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe = torch.zeros(max_len, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 16])"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1])"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position = torch.arange(0, max_len).unsqueeze(1)\n",
    "position.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8])"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "div_term = torch.exp(torch.arange(0, dim, 2) * -(math.log(10000.0) / dim))\n",
    "div_term.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 8])"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sin(position * div_term).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe[:, 0::2] = torch.sin(position.float() * div_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe[:, 1::2] = torch.cos(position.float() * div_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 16])"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1, 16])"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe = pe.unsqueeze(1)\n",
    "pe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1, 16])"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(10, 1, 16)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0599,  1.7412,  0.9173,  1.3884,  0.6826, -0.6830,  0.8005, -0.4316,\n",
       "          0.1788,  0.0446, -1.1409, -0.8781,  0.8874,  0.7887,  0.3160, -1.0217]])"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.]])"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0599,  2.7412,  0.9173,  2.3884,  0.6826,  0.3170,  0.8005,  0.5684,\n",
       "          0.1788,  1.0446, -1.1409,  0.1219,  0.8874,  1.7887,  0.3160, -0.0217]])"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x + pe[:x.size(0)])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 16])"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe = pe.unsqueeze(0)\n",
    "pe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 16])"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(1, 10, 16)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 16])"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x + (pe[:, :x.size(1), :])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 16])"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x + pe).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16])"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe[:, 1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_mask(lengths, max_len=None):\n",
    "    \"\"\"\n",
    "    Creates a boolean mask from sequence lengths.\n",
    "    \"\"\"\n",
    "    batch_size = lengths.numel()\n",
    "    max_len = max_len or lengths.max()\n",
    "    return (torch.arange(0, max_len, device=lengths.device)\n",
    "            .type_as(lengths)\n",
    "            .repeat(batch_size, 1)\n",
    "            .lt(lengths.unsqueeze(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (batch_size, )\n",
    "# 每个元素为序列的长度\n",
    "ts = torch.Tensor([5, 4, 3, 2])\n",
    "ts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 5])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = ~sequence_mask(ts).unsqueeze(1)\n",
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[False, False, False, False, False]],\n",
       "\n",
       "        [[False, False, False, False,  True]],\n",
       "\n",
       "        [[False, False, False,  True,  True]],\n",
       "\n",
       "        [[False, False,  True,  True,  True]]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 1, 5])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (batch_size, 1, 1, seq_len)\n",
    "mask = mask.unsqueeze(1)  # [B, 1, 1, T_values]\n",
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 5, 5])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (batch_size, head_count, query_len, key_len)\n",
    "score = torch.randint(1, 10, (4, 8, 5, 5))\n",
    "score.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 5, 5])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = score.masked_fill(mask, -1e18)\n",
    "scores.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsequent_mask(size):\n",
    "    \"Mask out subsequent positions.\"\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "    return torch.from_numpy(subsequent_mask) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True,  True,  True,  True,  True],\n",
       "        [ True,  True,  True,  True, False],\n",
       "        [ True,  True,  True, False, False],\n",
       "        [ True,  True, False, False, False]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_mask(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True,  True,  True,  True,  True],\n",
       "        [ True,  True,  True,  True, False],\n",
       "        [ True,  True,  True, False, False],\n",
       "        [ True,  True, False, False, False]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_mask(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
