{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [google-research/albert: ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://github.com/google-research/albert)\n",
    "\n",
    "For blog:\n",
    "\n",
    "[ALBERT 论文+代码笔记 | Yam](https://yam.gift/2020/05/10/Paper/2020-05-10-ALBERT/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlbertConfig:\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        embedding_size=128,\n",
    "        hidden_size=4096,\n",
    "        num_hidden_layers=12,\n",
    "        num_hidden_groups=1,\n",
    "        num_attention_heads=64,\n",
    "        intermediate_size=16384,\n",
    "        inner_group_num=1,\n",
    "        down_scale_factor=1,\n",
    "        hidden_act=\"gelu\",\n",
    "        hidden_dropout_prob=0,\n",
    "        attention_probs_dropout_prob=0,\n",
    "        max_position_embeddings=512,\n",
    "        type_vocab_size=2,\n",
    "        initializer_range=0.02):\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.num_hidden_groups = num_hidden_groups\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.inner_group_num = inner_group_num\n",
    "        self.down_scale_factor = down_scale_factor\n",
    "        self.hidden_act = hidden_act\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.hidden_dropout_prob = hidden_dropout_prob\n",
    "        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.type_vocab_size = type_vocab_size\n",
    "        self.initializer_range = initializer_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AlbertConfig(vocab_size=32000, \n",
    "                      hidden_size=512, \n",
    "                      num_hidden_layers=8, \n",
    "                      num_attention_heads=6, \n",
    "                      intermediate_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import copy\n",
    "import json\n",
    "import math\n",
    "import re\n",
    "import numpy as np\n",
    "import six\n",
    "from six.moves import range\n",
    "import tensorflow.compat.v1 as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlbertModel:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config,\n",
    "        is_training,\n",
    "        input_ids,\n",
    "        input_mask=None,\n",
    "        token_type_ids=None,\n",
    "        use_one_hot_embeddings=False,\n",
    "        use_einsum=True,\n",
    "        scope=None):\n",
    "        \n",
    "        config = copy.deepcopy(config)\n",
    "        input_shape = get_shape_list(input_ids, expected_rank=2)\n",
    "        batch_size, seq_length = input_shape\n",
    "        \n",
    "        with tf.variable_scope(scope, default_name=\"bert\"):\n",
    "            with tf.variable_scope(\"embeddings\"):\n",
    "                self.word_embedding_output, self.output_embedding_table = embedding_lookup(\n",
    "                    input_ids=input_ids, \n",
    "                    vocab_size=config.vocab_size, \n",
    "                    embedding_size=config.embedding_size,\n",
    "                    initializer_range=config.initializer_range,\n",
    "                    word_embedding_name=\"word_embeddings\",\n",
    "                    use_one_hot_embeddings=use_one_hot_embeddings)\n",
    "\n",
    "                # Add positional embeddings and token type embeddings, \n",
    "                # then layer normalize and perform dropout.\n",
    "                self.embedding_output = embedding_postprocessor(\n",
    "                    input_tensor=self.word_embedding_output,\n",
    "                    use_token_type=True,\n",
    "                    token_type_ids=token_type_ids,\n",
    "                    token_type_vocab_size=config.type_vocab_size,\n",
    "                    token_type_embedding_name=\"token_type_embeddings\",\n",
    "                    use_position_embeddings=True,\n",
    "                    position_embedding_name=\"position_embeddings\",\n",
    "                    initializer_range=config.initializer_range,\n",
    "                    max_position_embeddings=config.max_position_embeddings,\n",
    "                    dropout_prob=config.hidden_dropout_prob,\n",
    "                    use_one_hot_embeddings=use_one_hot_embeddings)\n",
    "            \n",
    "            with tf.variable_scope(\"encoder\"):\n",
    "                # Run the stacked transformer.\n",
    "                # `sequence_output` shape = [batch_size, seq_length, hidden_size].\n",
    "                self.all_encoder_layers = transformer_model(\n",
    "                    input_tensor=self.embedding_output,\n",
    "                    attention_mask=input_mask,\n",
    "                    hidden_size=config.hidden_size,\n",
    "                    num_hidden_layers=config.num_hidden_layers,\n",
    "                    num_hidden_groups=config.num_hidden_groups,\n",
    "                    num_attention_heads=config.num_attention_heads,\n",
    "                    intermediate_size=config.intermediate_size,\n",
    "                    inner_group_num=config.inner_group_num,\n",
    "                    intermediate_act_fn=get_activation(config.hidden_act),\n",
    "                    hidden_dropout_prob=config.hidden_dropout_prob,\n",
    "                    attention_probs_dropout_prob=config.attention_probs_dropout_prob,\n",
    "                    initializer_range=config.initializer_range,\n",
    "                    do_return_all_layers=True,\n",
    "                    use_einsum=use_einsum)\n",
    "        \n",
    "            self.sequence_output = self.all_encoder_layers[-1]\n",
    "            # The \"pooler\" converts the encoded sequence tensor of shape\n",
    "            # [batch_size, seq_length, hidden_size] to a tensor of shape\n",
    "            # [batch_size, hidden_size]. This is necessary for segment-level\n",
    "            # (or segment-pair-level) classification tasks where we need a fixed\n",
    "            # dimensional representation of the segment.\n",
    "            with tf.variable_scope(\"pooler\"):\n",
    "                # We \"pool\" the model by simply taking the hidden state corresponding\n",
    "                # to the first token. We assume that this has been pre-trained\n",
    "                first_token_tensor = tf.squeeze(self.sequence_output[:, 0:1, :], axis=1)\n",
    "                self.pooled_output = tf.layers.dense(\n",
    "                    first_token_tensor,\n",
    "                    config.hidden_size,\n",
    "                    activation=tf.tanh,\n",
    "                    kernel_initializer=create_initializer(config.initializer_range))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (batch_size, seq_length)\n",
    "input_ids = tf.constant([[31, 51, 99], [15, 5, 0]])\n",
    "input_mask = tf.constant([[1, 1, 1], [1, 1, 0]])\n",
    "token_type_ids = tf.constant([[0, 0, 1], [0, 0, 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_lookup(input_ids,\n",
    "                     vocab_size,\n",
    "                     embedding_size=128,\n",
    "                     word_embedding_name=\"word_embeddings\"):\n",
    "    # This function assumes that the input is of shape [batch_size, seq_length, num_inputs].\n",
    "    # If the input is a 2D tensor of shape [batch_size, seq_length], we\n",
    "    # reshape to [batch_size, seq_length, 1].\n",
    "    if input_ids.shape.ndims == 2:\n",
    "        # [batch_size, seq_length, 1]\n",
    "        input_ids = tf.expand_dims(input_ids, axis=[-1])\n",
    "    \n",
    "    embedding_table = tf.get_variable(\n",
    "        name=word_embedding_name,\n",
    "        shape=[vocab_size, embedding_size],\n",
    "        initializer=create_initializer(0.02))\n",
    "    \n",
    "    output = tf.nn.embedding_lookup(embedding_table, input_ids)\n",
    "    print(output.shape)\n",
    "    input_shape = get_shape_list(input_ids)\n",
    "    output = tf.reshape(output, input_shape[0:-1] + [input_shape[-1] * embedding_size])\n",
    "    return (output, embedding_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.shape.ndims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 3, 1])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.expand_dims(input_ids, axis=[-1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = get_shape_list(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3, 1, 128)\n"
     ]
    }
   ],
   "source": [
    "word_embedding_output, output_embedding_table = embedding_lookup(input_ids, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([100, 128])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_embedding_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 3, 128])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embedding_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Postprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_postprocessor(input_tensor,\n",
    "                            token_type_ids,\n",
    "                            token_type_vocab_size=2,\n",
    "                            max_position_embeddings=512,\n",
    "                            dropout_prob=0.1,\n",
    "                            token_type_embedding_name=\"token_type_embeddings\",\n",
    "                            position_embedding_name=\"position_embeddings\"):\n",
    "    input_shape = get_shape_list(input_tensor, expected_rank=3)\n",
    "    batch_size, seq_length, width = input_shape\n",
    "    \n",
    "    output = input_tensor\n",
    "    token_type_table = tf.get_variable(\n",
    "        token_type_embedding_name,\n",
    "        shape=[token_type_vocab_size, width],\n",
    "        initializer=create_initializer(0.02)\n",
    "    )\n",
    "    # \n",
    "    token_type_embeddings = tf.nn.embedding_lookup(token_type_table, token_type_ids)\n",
    "    print(token_type_embeddings.shape)\n",
    "    output += token_type_embeddings\n",
    "    \n",
    "    full_position_embeddings = tf.get_variable(\n",
    "        name=position_embedding_name,\n",
    "        shape=[max_position_embeddings, width],\n",
    "        initializer=create_initializer(0.02))\n",
    "    \n",
    "    position_embeddings = tf.slice(full_position_embeddings, [0, 0], [seq_length, -1])\n",
    "    num_dims = len(output.shape.as_list())\n",
    "\n",
    "    position_broadcast_shape = []\n",
    "    for _ in range(num_dims - 2):\n",
    "        position_broadcast_shape.append(1)\n",
    "    position_broadcast_shape.extend([seq_length, width])\n",
    "    position_embeddings = tf.reshape(position_embeddings, position_broadcast_shape)\n",
    "    print(position_embeddings.shape)\n",
    "    output += position_embeddings\n",
    "    \n",
    "    output = layer_norm_and_dropout(output, dropout_prob)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_position_embeddings = tf.get_variable(\n",
    "        name=\"position_embedding_name\",\n",
    "        shape=[512, 128],\n",
    "        initializer=create_initializer(0.02))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([512, 128])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_position_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 128]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_broadcast_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "position_embeddings = tf.slice(full_position_embeddings, [0, 0], [3, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([3, 128])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_type_ids = tf.zeros(shape=[2, 3], dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3, 128)\n",
      "(1, 3, 128)\n"
     ]
    }
   ],
   "source": [
    "embedding_output = embedding_postprocessor(word_embedding_output, token_type_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 3, 128])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_model(input_tensor,\n",
    "                      attention_mask=None,\n",
    "                      hidden_size=768,\n",
    "                      num_hidden_layers=12,\n",
    "                      num_hidden_groups=1,\n",
    "                      num_attention_heads=12,\n",
    "                      intermediate_size=3072,\n",
    "                      inner_group_num=1,\n",
    "                      intermediate_act_fn=\"gelu\",\n",
    "                      hidden_dropout_prob=0.1,\n",
    "                      attention_probs_dropout_prob=0.1,\n",
    "                      initializer_range=0.02,\n",
    "                      do_return_all_layers=False,\n",
    "                      use_einsum=True):\n",
    "    attention_head_size = hidden_size // num_attention_heads\n",
    "    input_shape = get_shape_list(input_tensor, expected_rank=3)\n",
    "    input_width = input_shape[2]\n",
    "    \n",
    "    all_layer_outputs = []\n",
    "    \n",
    "    if input_width != hidden_size:\n",
    "        # 本文的情况（第一个调整点）\n",
    "        prev_output = dense_layer_2d(\n",
    "            input_tensor, hidden_size, create_initializer(initializer_range),\n",
    "            None, use_einsum=use_einsum, name=\"embedding_hidden_mapping_in\")\n",
    "    else:\n",
    "        # 正常情况（如 Bert）\n",
    "        prev_output = input_tensor\n",
    "    \n",
    "    with tf.variable_scope(\"transformer\", reuse=tf.AUTO_REUSE):\n",
    "        for layer_idx in range(num_hidden_layers):\n",
    "            group_idx = int(layer_idx / num_hidden_layers * num_hidden_groups)\n",
    "            with tf.variable_scope(\"group_%d\" % group_idx):\n",
    "                with tf.name_scope(\"layer_%d\" % layer_idx):\n",
    "                    layer_output = prev_output\n",
    "                    for inner_group_idx in range(inner_group_num):\n",
    "                        with tf.variable_scope(\"inner_group_%d\" % inner_group_idx):\n",
    "                            layer_output = attention_ffn_block(\n",
    "                                layer_input=layer_output,\n",
    "                                hidden_size=hidden_size,\n",
    "                                attention_mask=attention_mask,\n",
    "                                num_attention_heads=num_attention_heads,\n",
    "                                attention_head_size=attention_head_size,\n",
    "                                attention_probs_dropout_prob=attention_probs_dropout_prob,\n",
    "                                intermediate_size=intermediate_size,\n",
    "                                intermediate_act_fn=intermediate_act_fn,\n",
    "                                initializer_range=initializer_range,\n",
    "                                hidden_dropout_prob=hidden_dropout_prob,\n",
    "                                use_einsum=use_einsum)\n",
    "                            prev_output = layer_output\n",
    "                            all_layer_outputs.append(layer_output)\n",
    "    if do_return_all_layers:\n",
    "        return all_layer_outputs\n",
    "    else:\n",
    "        return all_layer_outputs[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 3, 128])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 3, 768])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prev_output = dense_layer_2d(\n",
    "            embedding_output, 768, create_initializer(0.02),\n",
    "            None, use_einsum=True, name=\"embedding_hidden_mapping_in\")\n",
    "prev_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### attention ffn block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_ffn_block(layer_input,\n",
    "                        attention_mask,\n",
    "                        hidden_size=768,\n",
    "                        num_attention_heads=12,\n",
    "                        attention_head_size=64,\n",
    "                        attention_probs_dropout_prob=0.1,\n",
    "                        intermediate_size=3072,\n",
    "                        intermediate_act_fn=\"gleu\",\n",
    "                        initializer_range=0.02,\n",
    "                        hidden_dropout_prob=0.1,\n",
    "                        use_einsum=True):\n",
    "    with tf.variable_scope(\"attention_1\"):\n",
    "        with tf.variable_scope(\"self\"):\n",
    "            attention_output = attention_layer(\n",
    "                from_tensor=layer_input,\n",
    "                to_tensor=layer_input,\n",
    "                attention_mask=attention_mask,\n",
    "                num_attention_heads=num_attention_heads,\n",
    "                attention_probs_dropout_prob=attention_probs_dropout_prob,\n",
    "                initializer_range=initializer_range,\n",
    "                use_einsum=use_einsum\n",
    "            )\n",
    "        # Run a linear projection of `hidden_size` then add a residual\n",
    "        # with `layer_input`.\n",
    "        with tf.variable_scope(\"output\"):\n",
    "            attention_output = dense_layer_3d_proj(\n",
    "                attention_output,\n",
    "                hidden_size,\n",
    "                attention_head_size,\n",
    "                create_initializer(initializer_range),\n",
    "                None,\n",
    "                use_einsum=use_einsum,\n",
    "                name=\"dense\"\n",
    "            )\n",
    "            attention_output = dropout(attention_output, hidden_dropout_prob)\n",
    "    attention_output = layer_norm(attention_output + layer_input)\n",
    "    \n",
    "    with tf.variable_scope(\"ffn_1\"):\n",
    "        with tf.variable_scope(\"intermediate\"):\n",
    "            intermediate_output = dense_layer_2d(\n",
    "                attention_output,\n",
    "                intermediate_size,\n",
    "                create_initializer(initializer_range),\n",
    "                intermediate_act_fn,\n",
    "                use_einsum=use_einsum,\n",
    "                num_attention_heads=num_attention_heads,\n",
    "                name=\"dense\")\n",
    "        with tf.variable_scope(\"output\"):\n",
    "            ffn_output = dense_layer_2d(\n",
    "                intermediate_output,\n",
    "                hidden_size,\n",
    "                create_initializer(initializer_range),\n",
    "                None,\n",
    "                use_einsum=use_einsum,\n",
    "                num_attention_heads=num_attention_heads,\n",
    "                name=\"dense\")\n",
    "        ffn_output = dropout(ffn_output, hidden_dropout_prob)\n",
    "    ffn_output = layer_norm(ffn_output + attention_output)\n",
    "    return ffn_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 768]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_shape_list(prev_output, expected_rank=[2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_layer(from_tensor,\n",
    "                    to_tensor,\n",
    "                    attention_mask,\n",
    "                    num_attention_heads=12,\n",
    "                    query_act=None,\n",
    "                    key_act=None,\n",
    "                    value_act=None,\n",
    "                    attention_probs_dropout_prob=0.1,\n",
    "                    initializer_range=0.02,\n",
    "                    batch_size=None,\n",
    "                    from_seq_length=None,\n",
    "                    to_seq_length=None,\n",
    "                    use_einsum=True):\n",
    "    \n",
    "    # (batch_size, seq_length, hidden_size)\n",
    "    from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])\n",
    "    to_shape = get_shape_list(to_tensor, expected_rank=[2, 3])\n",
    "    # 768/12 = 64\n",
    "    size_per_head = int(from_shape[2]/num_attention_heads)\n",
    "    \n",
    "    batch_size = from_shape[0]\n",
    "    from_seq_length = from_shape[1]\n",
    "    to_seq_length = to_shape[1]\n",
    "\n",
    "\n",
    "    # `query_layer` = [B, F, N, H]\n",
    "    q = dense_layer_3d(from_tensor, num_attention_heads, size_per_head, \n",
    "                       create_initializer(initializer_range), query_act, use_einsum, \"query\")\n",
    "\n",
    "    # `key_layer` = [B, T, N, H]\n",
    "    k = dense_layer_3d(to_tensor, num_attention_heads, size_per_head,\n",
    "                       create_initializer(initializer_range), key_act, use_einsum, \"key\")\n",
    "    # `value_layer` = [B, T, N, H]\n",
    "    v = dense_layer_3d(to_tensor, num_attention_heads, size_per_head, \n",
    "                       create_initializer(initializer_range), value_act, use_einsum, \"value\")\n",
    "    \n",
    "    q = tf.transpose(q, [0, 2, 1, 3])\n",
    "    k = tf.transpose(k, [0, 2, 1, 3])\n",
    "    v = tf.transpose(v, [0, 2, 1, 3])\n",
    "    \n",
    "    if attention_mask is not None:\n",
    "        attention_mask = tf.reshape(attention_mask, [batch_size, 1, to_seq_length, 1])\n",
    "    \n",
    "    # 'new_embeddings = [B, N, F, H]'\n",
    "    new_embeddings = dot_product_attention(q, k, v, attention_mask, attention_probs_dropout_prob)\n",
    "    return tf.transpose(new_embeddings, [0, 2, 1, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 3, 12, 64])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = dense_layer_3d(prev_output, 12, 64, create_initializer(0.02), None, True, \"query\")\n",
    "q.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dot production attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 12, 64]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from_shape=get_shape_list(q)\n",
    "from_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_product_attention(q, k, v, mask, dropout_rate=0.1):\n",
    "    # (seq_length, num_heads, q_length, kv_length)\n",
    "    logits = tf.matmul(q, k, transpose_b=True)  # [..., length_q, length_kv]\n",
    "    logits = tf.multiply(logits, 1.0 / math.sqrt(float(get_shape_list(q)[-1])))\n",
    "    if mask is not None:\n",
    "        # `attention_mask` = [B, T]\n",
    "        from_shape = get_shape_list(q)\n",
    "        broadcast_ones = tf.ones([from_shape[0], 1, from_shape[2], 1], tf.float32)\n",
    "        mask = tf.matmul(broadcast_ones,\n",
    "                         tf.cast(mask, tf.float32), transpose_b=True)\n",
    "\n",
    "        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n",
    "        # masked positions, this operation will create a tensor which is 0.0 for\n",
    "        # positions we want to attend and -10000.0 for masked positions.\n",
    "        adder = (1.0 - mask) * -10000.0\n",
    "\n",
    "        # Since we are adding it to the raw scores before the softmax, this is\n",
    "        # effectively the same as removing these entirely.\n",
    "        logits += adder\n",
    "    else:\n",
    "        adder = 0.0\n",
    "\n",
    "    attention_probs = tf.nn.softmax(logits, name=\"attention_probs\")\n",
    "    attention_probs = dropout(attention_probs, dropout_rate)\n",
    "    return tf.matmul(attention_probs, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 12, 3, 64])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = tf.transpose(q, [0, 2, 1, 3])\n",
    "q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 1, 12, 1])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "broadcast_ones = tf.ones([from_shape[0], 1, from_shape[2], 1], tf.float32)\n",
    "broadcast_ones.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 1, 3, 1])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask = tf.reshape(input_mask, [2, 1, 3, 1])\n",
    "attention_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 1, 12, 3])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias = tf.matmul(broadcast_ones, tf.cast(attention_mask, tf.float32), transpose_b=True)\n",
    "bias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=int32, numpy=\n",
       "array([[1, 1, 1],\n",
       "       [1, 1, 0]], dtype=int32)>"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 1, 12, 3), dtype=float32, numpy=\n",
       "array([[[[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]]],\n",
       "\n",
       "\n",
       "       [[[1., 1., 0.],\n",
       "         [1., 1., 0.],\n",
       "         [1., 1., 0.],\n",
       "         [1., 1., 0.],\n",
       "         [1., 1., 0.],\n",
       "         [1., 1., 0.],\n",
       "         [1., 1., 0.],\n",
       "         [1., 1., 0.],\n",
       "         [1., 1., 0.],\n",
       "         [1., 1., 0.],\n",
       "         [1., 1., 0.],\n",
       "         [1., 1., 0.]]]], dtype=float32)>"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 1, 12, 3), dtype=float32, numpy=\n",
       "array([[[[    -0.,     -0.,     -0.],\n",
       "         [    -0.,     -0.,     -0.],\n",
       "         [    -0.,     -0.,     -0.],\n",
       "         [    -0.,     -0.,     -0.],\n",
       "         [    -0.,     -0.,     -0.],\n",
       "         [    -0.,     -0.,     -0.],\n",
       "         [    -0.,     -0.,     -0.],\n",
       "         [    -0.,     -0.,     -0.],\n",
       "         [    -0.,     -0.,     -0.],\n",
       "         [    -0.,     -0.,     -0.],\n",
       "         [    -0.,     -0.,     -0.],\n",
       "         [    -0.,     -0.,     -0.]]],\n",
       "\n",
       "\n",
       "       [[[    -0.,     -0., -10000.],\n",
       "         [    -0.,     -0., -10000.],\n",
       "         [    -0.,     -0., -10000.],\n",
       "         [    -0.,     -0., -10000.],\n",
       "         [    -0.,     -0., -10000.],\n",
       "         [    -0.,     -0., -10000.],\n",
       "         [    -0.,     -0., -10000.],\n",
       "         [    -0.,     -0., -10000.],\n",
       "         [    -0.,     -0., -10000.],\n",
       "         [    -0.,     -0., -10000.],\n",
       "         [    -0.,     -0., -10000.],\n",
       "         [    -0.,     -0., -10000.]]]], dtype=float32)>"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adder = (1.0 - bias) * -10000.0\n",
    "adder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 12, 3, 3])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = tf.matmul(q, q, transpose_b=True)  # [..., length_q, length_kv]\n",
    "logits = tf.multiply(logits, 1.0 / math.sqrt(float(get_shape_list(q)[-1])))\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最后一层\n",
    "all_layer_outputs = transformer_model(embedding_output, input_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 3, 768])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_layer_outputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 768])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_token_tensor = tf.squeeze(all_layer_outputs[:, 0:1, :], axis=1)\n",
    "first_token_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 768])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool = tf.keras.layers.Dense(768, activation=tf.tanh, kernel_initializer=create_initializer(0.02))\n",
    "pooled_output = pool(first_token_tensor)\n",
    "pooled_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SOP Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_weights = tf.get_variable(\n",
    "    \"output_weights\", \n",
    "    shape=[2, 768],\n",
    "    initializer=create_initializer(0.02))\n",
    "output_bias = tf.get_variable(\n",
    "    \"output_bias\", shape=[2], initializer=tf.zeros_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'output_weights:0' shape=(2, 768) dtype=float32, numpy=\n",
       "array([[ 0.00258045,  0.00957985, -0.00063457, ..., -0.00678589,\n",
       "         0.01690069,  0.02294636],\n",
       "       [ 0.00786927,  0.0039197 , -0.00218187, ..., -0.026448  ,\n",
       "        -0.03057633, -0.01545808]], dtype=float32)>"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'output_bias:0' shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)>"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 2])"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = tf.matmul(pooled_output, output_weights, transpose_b=True)\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 2])"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = tf.nn.bias_add(logits, output_bias)\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[ 0.28498146, -0.3474557 ],\n",
       "       [ 0.38374817, -0.06567392]], dtype=float32)>"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 2])"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
    "log_probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[-0.426114  , -1.0585512 ],\n",
       "       [-0.49347395, -0.942896  ]], dtype=float32)>"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[0.6530419 , 0.34695813],\n",
       "       [0.6105018 , 0.38949817]], dtype=float32)>"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.nn.softmax(logits, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 1])"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = tf.constant([[0], [1]])\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2])"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = tf.reshape(labels, [-1])\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 2])"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_labels = tf.one_hot(labels, depth=2, dtype=tf.float32)\n",
    "one_hot_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[1., 0.],\n",
       "       [0., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
    "loss = tf.reduce_mean(per_example_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2])"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "per_example_loss.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([0.426114, 0.942896], dtype=float32)>"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "per_example_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.684505>"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_norm(input_tensor, name=None):\n",
    "    return tf.keras.layers.LayerNormalization(name=name,axis=-1,epsilon=1e-12,dtype=tf.float32)(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout(input_tensor, dropout_prob):\n",
    "    if dropout_prob is None or dropout_prob == 0.0:\n",
    "        return input_tensor\n",
    "    output = tf.nn.dropout(input_tensor, rate=dropout_prob)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_norm_and_dropout(input_tensor, dropout_prob, name=None):\n",
    "    output_tensor = layer_norm(input_tensor, name)\n",
    "    output_tensor = dropout(output_tensor, dropout_prob)\n",
    "    return output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shape_list(tensor, expected_rank=None, name=None):\n",
    "    return tensor.shape.as_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_initializer(initializer_range=0.02):\n",
    "    return tf.truncated_normal_initializer(stddev=initializer_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_layer_3d_proj(input_tensor,\n",
    "                        hidden_size,\n",
    "                        head_size,\n",
    "                        initializer,\n",
    "                        activation,\n",
    "                        use_einsum,\n",
    "                        name=None):\n",
    "    input_shape = get_shape_list(input_tensor)\n",
    "    num_attention_heads = input_shape[2]\n",
    "    with tf.variable_scope(name):\n",
    "        w = tf.get_variable(\n",
    "            name=\"kernel\", shape=[num_attention_heads * head_size, hidden_size], initializer=initializer)\n",
    "    w = tf.reshape(w, [num_attention_heads, head_size, hidden_size])\n",
    "    b = tf.get_variable(\n",
    "        name=\"bias\", shape=[hidden_size], initializer=tf.zeros_initializer)\n",
    "    if use_einsum:\n",
    "        ret = tf.einsum(\"BFND,NDH->BFH\", input_tensor, w)\n",
    "    else:\n",
    "        ret = einsum_via_matmul(input_tensor, w, 2)\n",
    "    ret += b\n",
    "    if activation is not None:\n",
    "        return activation(ret)\n",
    "    else:\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_layer_3d(input_tensor,\n",
    "                   num_attention_heads,\n",
    "                   head_size,\n",
    "                   initializer,\n",
    "                   activation,\n",
    "                   use_einsum,\n",
    "                   name=None):\n",
    "    input_shape = get_shape_list(input_tensor)\n",
    "    hidden_size = input_shape[2]\n",
    "    \n",
    "    with tf.variable_scope(name):\n",
    "        w = tf.get_variable(\n",
    "            name=\"kernel\", shape=[hidden_size, num_attention_heads * head_size], initializer=initializer)\n",
    "    w = tf.reshape(\n",
    "        w, [hidden_size, num_attention_heads, head_size])\n",
    "    b = tf.get_variable(\n",
    "        name=\"bias\", shape=[num_attention_heads * head_size], initializer=tf.zeros_initializer)\n",
    "    b = tf.reshape(b, [num_attention_heads, head_size])\n",
    "    if use_einsum:\n",
    "        ret = tf.einsum(\"BFH,HND->BFND\", input_tensor, w)\n",
    "    else:\n",
    "        ret = einsum_via_matmul(input_tensor, w, 1)\n",
    "    ret += b\n",
    "    if activation is not None:\n",
    "        return get_activation(activation)(ret)\n",
    "    else:\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_layer_2d(input_tensor,\n",
    "                   output_size,\n",
    "                   initializer,\n",
    "                   activation,\n",
    "                   use_einsum,\n",
    "                   num_attention_heads=1,\n",
    "                   name=None):\n",
    "    input_shape = get_shape_list(input_tensor)\n",
    "    hidden_size = input_shape[2]\n",
    "    with tf.variable_scope(name):\n",
    "        w = tf.get_variable(name=\"kernel\", shape=[hidden_size, output_size], initializer=initializer)\n",
    "        b = tf.get_variable(name=\"bias\", shape=[output_size], initializer=tf.zeros_initializer)\n",
    "    if use_einsum:\n",
    "        ret = tf.einsum(\"BFH,HO->BFO\", input_tensor, w)\n",
    "    else:\n",
    "        ret = tf.matmul(input_tensor, w)\n",
    "    ret += b\n",
    "    if activation is not None:\n",
    "        return get_activation(activation)(ret)\n",
    "    else:\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gelu(x):\n",
    "    \"\"\"Gaussian Error Linear Unit.\n",
    "\n",
    "    This is a smoother version of the RELU.\n",
    "    Original paper: https://arxiv.org/abs/1606.08415\n",
    "    Args:\n",
    "    x: float Tensor to perform activation.\n",
    "\n",
    "    Returns:\n",
    "    `x` with the GELU activation applied.\n",
    "    \"\"\"\n",
    "    cdf = 0.5 * (1.0 + tf.tanh((np.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3)))))\n",
    "    return x * cdf\n",
    "\n",
    "\n",
    "def get_activation(activation_string):\n",
    "    if not isinstance(activation_string, six.string_types):\n",
    "        return activation_string\n",
    "\n",
    "    if not activation_string:\n",
    "        return None\n",
    "\n",
    "    act = activation_string.lower()\n",
    "    if act == \"linear\":\n",
    "        return None\n",
    "    elif act == \"relu\":\n",
    "        return tf.nn.relu\n",
    "    elif act == \"gelu\":\n",
    "        return gelu\n",
    "    elif act == \"tanh\":\n",
    "        return tf.tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 2, 3])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.constant([[[1, 2, 3], [4, 5, 6]]])\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([3, 1])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = tf.constant([[1], [2], [3]])\n",
    "w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 2, 1), dtype=int32, numpy=\n",
       "array([[[14],\n",
       "        [32]]], dtype=int32)>"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.matmul(x, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 2, 1), dtype=int32, numpy=\n",
       "array([[[14],\n",
       "        [32]]], dtype=int32)>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.einsum(\"BFH,HO->BFO\", x, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 2, 1), dtype=int32, numpy=\n",
       "array([[[14],\n",
       "        [32]]], dtype=int32)>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.tensordot(x, w, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.54545455, 0.27272727, 0.18181818])"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pvals = 1. / np.arange(1, 3 + 1)\n",
    "pvals /= pvals.sum(keepdims=True)\n",
    "pvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.66666667, 0.33333333])"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 2\n",
    "p=pvals[:i] /pvals[:i].sum(keepdims=True)\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3])"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngrams = np.arange(1, 3 + 1, dtype=np.int64)\n",
    "ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = np.random.choice(ngrams[:i], p=pvals[:i] /pvals[:i].sum(keepdims=True))\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "rng = random.Random(12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/HaoShaochun/Documents/Study/DL-Models/albert\n"
     ]
    }
   ],
   "source": [
    "cd ~/Documents/Study/DL-Models/albert/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tokenization.FullTokenizer(\n",
    "      vocab_file=\"vocab.txt\", do_lower_case=True,\n",
    "      spm_model_file=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_a = \"Text should be one-sentence-per-line, with empty lines between documents.\".split()\n",
    "tokens_b = \"This sample text is public domain and was randomly selected from Project Guttenberg.\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "tokens.append(\"[CLS]\")\n",
    "for token in tokens_a:\n",
    "    tokens.append(token)\n",
    "tokens.append(\"[SEP]\")\n",
    "for token in tokens_b:\n",
    "    tokens.append(token)\n",
    "tokens.append(\"[SEP]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "MaskedLmInstance = collections.namedtuple(\"MaskedLmInstance\",\n",
    "                                          [\"index\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_to_predict:  4\n",
      "[[[9]], [[9], [11]], [[9], [11], [12]]]\n",
      "[[[20]], [[20], [21]], [[20], [21], [22]]]\n",
      "[[[3]], [[3], [4]], [[3], [4], [5]]]\n"
     ]
    }
   ],
   "source": [
    "(output_tokens, \n",
    " masked_lm_positions, \n",
    " masked_lm_labels) = create_masked_lm_predictions(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9, 11, 12, 20]"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_lm_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['documents.', 'This', 'sample', 'selected']"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_lm_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] Text should be one-sentence-per-line, with empty lines between documents. [SEP] This sample text is public domain and was randomly selected from Project Guttenberg. [SEP]\n"
     ]
    }
   ],
   "source": [
    "print(\" \".join(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] Text should be one-sentence-per-line, with empty lines between [MASK] [SEP] [MASK] [MASK] text is public domain and was randomly 屿 from Project Guttenberg. [SEP]\n"
     ]
    }
   ],
   "source": [
    "print(\" \".join(output_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masked_lm_predictions(\n",
    "    tokens, \n",
    "    masked_lm_prob=0.15, \n",
    "    max_predictions_per_seq=20, \n",
    "    vocab_words=list(tokenizer.vocab.keys()), \n",
    "    rng=rng):\n",
    "    \n",
    "    cand_indexes = []\n",
    "    for (i, token) in enumerate(tokens):\n",
    "        if token == \"[CLS]\" or token == \"[SEP]\":\n",
    "            continue\n",
    "        cand_indexes.append([i])\n",
    "    \n",
    "    output_tokens = list(tokens)\n",
    "    masked_lm_positions = []\n",
    "    masked_lm_labels = []\n",
    "    num_to_predict = min(max_predictions_per_seq, \n",
    "                         max(1, int(round(len(tokens) * masked_lm_prob))))\n",
    "    \n",
    "    print(\"num_to_predict: \", num_to_predict)\n",
    "    ngrams = np.arange(1, 3 + 1, dtype=np.int64)\n",
    "    pvals = 1. / np.arange(1, 3 + 1)\n",
    "    pvals /= pvals.sum(keepdims=True)\n",
    "    \n",
    "    ngram_indexes = []\n",
    "    for idx in range(len(cand_indexes)):\n",
    "        ngram_index = []\n",
    "        for n in ngrams:\n",
    "            ngram_index.append(cand_indexes[idx:idx+n])\n",
    "        ngram_indexes.append(ngram_index)\n",
    "    rng.shuffle(ngram_indexes)\n",
    "    \n",
    "    masked_lms = []\n",
    "    covered_indexes = set()\n",
    "    for cand_index_set in ngram_indexes:\n",
    "        print(cand_index_set)\n",
    "        if len(masked_lms) >= num_to_predict:\n",
    "            break\n",
    "        n = np.random.choice(\n",
    "            ngrams[:len(cand_index_set)], \n",
    "            p=pvals[:len(cand_index_set)] / pvals[:len(cand_index_set)].sum(keepdims=True))\n",
    "        # [16, 17] = sum([[16], [17]], [])\n",
    "        index_set = sum(cand_index_set[n - 1], [])\n",
    "        for index in index_set:\n",
    "            masked_token = None\n",
    "            if rng.random() < 0.8:\n",
    "                masked_token = \"[MASK]\"\n",
    "            else:\n",
    "                if rng.random() < 0.5:\n",
    "                    masked_token = tokens[index]\n",
    "                else:\n",
    "                    masked_token = vocab_words[rng.randint(0, len(vocab_words) - 1)]\n",
    "            output_tokens[index] = masked_token\n",
    "            masked_lms.append(MaskedLmInstance(index=index, label=tokens[index]))\n",
    "    rng.shuffle(ngram_indexes)\n",
    "  \n",
    "    masked_lms = sorted(masked_lms, key=lambda x: x.index)\n",
    "\n",
    "    for p in masked_lms:\n",
    "        masked_lm_positions.append(p.index)\n",
    "        masked_lm_labels.append(p.label)\n",
    "    return (output_tokens, masked_lm_positions, masked_lm_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masked_lm_predictions(\n",
    "    tokens, \n",
    "    masked_lm_prob=0.15, \n",
    "    max_predictions_per_seq=20, \n",
    "    vocab_words=list(tokenizer.vocab.keys()), \n",
    "    rng=rng):\n",
    "    \n",
    "    cand_indexes = []\n",
    "    token_boundary = [0] * len(tokens)\n",
    "\n",
    "    for (i, token) in enumerate(tokens):\n",
    "        if token == \"[CLS]\" or token == \"[SEP]\":\n",
    "            token_boundary[i] = 1\n",
    "            continue\n",
    "        cand_indexes.append([i])\n",
    "        token_boundary[i] = 1\n",
    "    \n",
    "    output_tokens = list(tokens)\n",
    "    masked_lm_positions = []\n",
    "    masked_lm_labels = []\n",
    "    num_to_predict = min(max_predictions_per_seq, \n",
    "                         max(1, int(round(len(tokens) * masked_lm_prob))))\n",
    "    \n",
    "    print(\"num_to_predict: \", num_to_predict)\n",
    "    ngrams = np.arange(1, 3 + 1, dtype=np.int64)\n",
    "    pvals = 1. / np.arange(1, 3 + 1)\n",
    "    pvals /= pvals.sum(keepdims=True)\n",
    "    \n",
    "    ngram_indexes = []\n",
    "    for idx in range(len(cand_indexes)):\n",
    "        ngram_index = []\n",
    "        for n in ngrams:\n",
    "            ngram_index.append(cand_indexes[idx:idx+n])\n",
    "        ngram_indexes.append(ngram_index)\n",
    "    rng.shuffle(ngram_indexes)\n",
    "    \n",
    "    masked_lms = []\n",
    "    covered_indexes = set()\n",
    "    for cand_index_set in ngram_indexes:\n",
    "        print(\"cand_index_set\", cand_index_set)\n",
    "        if len(masked_lms) >= num_to_predict:\n",
    "            break\n",
    "        if not cand_index_set:\n",
    "            continue\n",
    "        for index_set in cand_index_set[0]:\n",
    "            print(index_set)\n",
    "            print(\"H?\")\n",
    "            for index in index_set:\n",
    "                print(index, covered_indexes)\n",
    "                if index in covered_indexes:\n",
    "                    continue\n",
    "\n",
    "        n = np.random.choice(\n",
    "            ngrams[:len(cand_index_set)], \n",
    "            p=pvals[:len(cand_index_set)] / pvals[:len(cand_index_set)].sum(keepdims=True))\n",
    "        # [16, 17] = sum([[16], [17]], [])\n",
    "        index_set = sum(cand_index_set[n - 1], [])\n",
    "        n -= 1\n",
    "        while len(masked_lms) + len(index_set) > num_to_predict:\n",
    "            print(\"I?\")\n",
    "            if n == 0:\n",
    "                break\n",
    "            index_set = sum(cand_index_set[n - 1], [])\n",
    "            n -= 1\n",
    "        if len(masked_lms) + len(index_set) > num_to_predict:\n",
    "            continue\n",
    "        is_any_index_covered = False\n",
    "        for index in index_set:\n",
    "            print(\"J?\")\n",
    "            if index in covered_indexes:\n",
    "                is_any_index_covered = True\n",
    "                break\n",
    "        if is_any_index_covered:\n",
    "            continue\n",
    "        for index in index_set:\n",
    "            covered_indexes.add(index)\n",
    "            masked_token = None\n",
    "            if rng.random() < 0.8:\n",
    "                masked_token = \"[MASK]\"\n",
    "            else:\n",
    "                if rng.random() < 0.5:\n",
    "                    masked_token = tokens[index]\n",
    "                else:\n",
    "                    masked_token = vocab_words[rng.randint(0, len(vocab_words) - 1)]\n",
    "            output_tokens[index] = masked_token\n",
    "            masked_lms.append(MaskedLmInstance(index=index, label=tokens[index]))\n",
    "    rng.shuffle(ngram_indexes)\n",
    "  \n",
    "    masked_lms = sorted(masked_lms, key=lambda x: x.index)\n",
    "\n",
    "    for p in masked_lms:\n",
    "        masked_lm_positions.append(p.index)\n",
    "        masked_lm_labels.append(p.label)\n",
    "    return (output_tokens, masked_lm_positions, masked_lm_labels, token_boundary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_to_predict:  4\n",
      "cand_index_set [[[12]], [[12], [13]], [[12], [13], [14]]]\n",
      "[12]\n",
      "H?\n",
      "12 set()\n",
      "J?\n",
      "J?\n",
      "cand_index_set [[[3]], [[3], [4]], [[3], [4], [5]]]\n",
      "[3]\n",
      "H?\n",
      "3 {12, 13}\n",
      "J?\n",
      "cand_index_set [[[22]], [[22], [23]], [[22], [23]]]\n",
      "[22]\n",
      "H?\n",
      "22 {3, 12, 13}\n",
      "J?\n",
      "cand_index_set [[[15]], [[15], [16]], [[15], [16], [17]]]\n"
     ]
    }
   ],
   "source": [
    "(output_tokens, \n",
    " masked_lm_positions, \n",
    " masked_lm_labels,\n",
    "token_boundary) = create_masked_lm_predictions(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 21, 22, 23]"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_lm_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['with', 'from', 'Project', 'Guttenberg.']"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_lm_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'Text',\n",
       " 'should',\n",
       " 'be',\n",
       " 'one-sentence-per-line,',\n",
       " '##聰',\n",
       " 'empty',\n",
       " 'lines',\n",
       " 'between',\n",
       " 'documents.',\n",
       " '[SEP]',\n",
       " 'This',\n",
       " 'sample',\n",
       " 'text',\n",
       " 'is',\n",
       " 'public',\n",
       " 'domain',\n",
       " 'and',\n",
       " 'was',\n",
       " 'randomly',\n",
       " 'selected',\n",
       " '[MASK]',\n",
       " '[MASK]',\n",
       " 'Guttenberg.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
